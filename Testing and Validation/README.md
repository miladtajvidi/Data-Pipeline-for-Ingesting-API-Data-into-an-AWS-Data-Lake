# 6. Testing and Validation

In this final section, we’ll focus on testing and validating the entire pipeline to ensure that each component works as expected. This includes verifying the Lambda function, S3 data storage, error handling, automation, and monitoring. We’ll also test various failure scenarios and ensure the data pipeline is robust and resilient to errors.

## 6.1 End-to-End Testing

The goal of end-to-end testing is to ensure the entire pipeline works seamlessly, from data ingestion to data storage in S3.

### 6.1.1 Manually Triggering the Lambda Function

We start by manually triggering the Lambda function to verify that it fetches data from the Coinbase API and stores it in S3.

* Triggering Lambda Manually using:
    * 1. AWS Console:
        * Go to the Lambda section in the AWS console.
        * Select the CoinbaseDataIngestion function.
        * Click on Test to manually invoke the function. You can create a test event using the default input provided by AWS or leave it empty.
        * Verify the output and check if the data is being stored in the correct S3 path.
    * 2. AWS CLI:
        * You can also manually invoke the Lambda function using the AWS CLI:
            ```bash
                    aws lambda invoke --function-name CoinbaseDataIngestion \
                    --payload '{}' output.txt
            ```
        * Check the output.txt file for the response, and then verify the S3 bucket for the presence of the fetched data.

### 6.1.2 Verifying S3 Data

We need to confirm that the data is properly stored in S3 in the expected folder structure.

* Verify Data in S3:
    * Go to the S3 bucket in the AWS console.
    * Navigate through the bucket structure (raw/coinbase/year/month/day/hour/).
    * Ensure that the data is in the correct format (e.g., JSON) and that the file is named appropriately (e.g., data.json).
* CLI Verification:
    * We can list the files in the S3 bucket using the AWS CLI:
        ``` bash
           aws s3 ls s3://my-bucket-name/raw/coinbase/
 
        ```


## 6.2 Test Error Handling and Retry Logic

Testing how the pipeline handles errors is crucial. We’ll simulate various errors (e.g., network failures, invalid data, API throttling) and ensure the pipeline recovers properly.

### 6.2.1 Simulating API Failure
We can temporarily block the API endpoint in the Lambda function to simulate an API failure and verify that retries and error handling work as expected.
* Modify the API endpoint to an incorrect URL (e.g., https://api.invalid-url.com), then manually trigger the Lambda function.
* Check the CloudWatch Logs for error details and ensure that the function retries (we can check if the function invoked multiple times).
    ```python
        # Simulating failure by modifying API URL
        url = "https://api.invalid-url.com"
    ```

### 6.2.2 Testing Rate Limiting and Backoff

We test how the Lambda function handles API rate limiting and simulate this by calling the API multiple times in a loop to trigger rate limits.
* We should expect an error code 429 Too Many Requests from the Coinbase API.
* We check CloudWatch Logs to see if exponential backoff is working correctly, and ensure that retries are attempted.

## 6.3 Monitoring and Alerts

We ensure that monitoring and alerts are properly set up so we get notified in case of any errors.

### 6.3.1 CloudWatch Logs

* Go to CloudWatch Logs and review the logs generated by the Lambda function.
* Verify that important metrics such as invocation errors, retries, and execution duration are logged.

### 6.3.2 CloudWatch Metrics

* Monitoring Lambda Metrics:
    * In the CloudWatch console, navigate to Metrics → Lambda.
    * Check Metrics like:
        * Invocations: Number of times the function was invoked.
        * Errors: Number of errors encountered.
        * Throttles: Number of times the function was throttled due to hitting concurrency limits.

### 6.3.3 Setting Up Alarms

If CloudWatch alarms aren’t already configured, we can set up alarms to notify us when certain thresholds are breached, such as errors or timeouts.
* Creating an Alarm for Lambda Errors:
    ```bash
        aws cloudwatch put-metric-alarm --alarm-name "CoinbaseLambdaErrors" \
        --metric-name "Errors" --namespace "AWS/Lambda" \
        --statistic "Sum" --period 300 --threshold 1 \
        --comparison-operator "GreaterThanOrEqualToThreshold" \
        --dimensions "Name=FunctionName,Value=CoinbaseDataIngestion" \
        --evaluation-periods 1 --alarm-actions arn:aws:sns:us-west-2:your-sns-topic

    ```
* We can create similar alarms for Execution Duration, Throttling, and more.
## 6.4 Test Data Validation
We can test how the Lambda function handles invalid or incomplete data from the API.
### 6.4.1 Invalid Data Testing
* Modify the API response to remove certain required fields or return invalid data types (if you have access to the API or mock it).
* Verify that the function logs an error or triggers an alert when data does not match the expected schema.
* Check CloudWatch Logs for validation errors, and ensure that the function does not store invalid data in S3.

## 6.5 Performance Testing
We can test how the pipeline handles increased loads by manually triggering multiple Lambda functions simultaneously.
### 6.5.1 Scalability and Concurrency Testing
* Invoke the Lambda function in parallel (e.g., by using a script or AWS CLI commands) to simulate heavy API usage.
* Ensure that the function handles concurrency limits without failures (AWS Lambda scales automatically, but we can configure reserved concurrency if needed).
## 6.6 Full End-to-End Testing Code

```python
    import json
    import boto3
    import requests
    from datetime import datetime
    import time

    def get_secret(secret_name):
        client = boto3.client('secretsmanager')
        response = client.get_secret_value(SecretId=secret_name)
        secret = json.loads(response['SecretString'])
        return secret['COINBASE_API_KEY']

    def lambda_handler(event, context):
        # Retrieve API key from Secrets Manager
        api_key = get_secret("prod/coinbase/api_key")
        
        # Set API endpoint and parameters
        url = "https://api.coinbase.com/v2/prices/spot"
        headers = {
            'Authorization': f'Bearer {api_key}'
        }
        params = {
            'currency': 'USD'
        }
        
        max_retries = 5
        for attempt in range(max_retries):
            try:
                response = requests.get(url, headers=headers, params=params)
                if response.status_code == 200:
                    data = response.json()

                    # Data validation (Example: Ensure 'data' field exists)
                    if 'data' not in data:
                        raise ValueError("Invalid data format: 'data' field missing")

                    # Prepare S3 path and store data
                    s3 = boto3.client('s3')
                    bucket_name = 'my-bucket-name'
                    now = datetime.utcnow()
                    key = f'raw/coinbase/{now.year}/{now.month}/{now.day}/{now.hour}/data.json'
                    
                    s3.put_object(
                        Bucket=bucket_name,
                        Key=key,
                        Body=json.dumps(data),
                        ContentType='application/json'
                    )
                    
                    return {
                        'statusCode': 200,
                        'body': json.dumps('Data successfully fetched and stored in S3')
                    }
                else:
                    raise Exception(f"API Error: {response.status_code}")
            
            except Exception as e:
                # Handle rate limiting or transient errors with exponential backoff
                if response.status_code == 429 or attempt < max_retries - 1:
                    time.sleep(2 ** attempt)
                else:
                    # Log error to CloudWatch and raise alert
                    print(f"Failed to fetch data: {str(e)}")
                    raise e


```

By completing this testing and validation phase, our data pipeline will be fully tested, automated, and ready for production.